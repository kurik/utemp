#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from httplib2 import Http
from apiclient.discovery import build
from oauth2client.file import Storage
from oauth2client.client import flow_from_clientsecrets
from oauth2client.tools import run_flow
from oauth2client import tools

import logging
import argparse
import sqlite3
from os.path import expanduser
#from multiprocessing import Pool

OAUTH2_SCOPE = 'https://www.googleapis.com/auth/spreadsheets'

# A helper formater for argpartse
class SmartFormatter(argparse.HelpFormatter):
    def _split_lines(self, text, width):
        if text.startswith('R|'):
            return text[2:].splitlines()  
        # this is the RawTextHelpFormatter._split_lines
        return argparse.HelpFormatter._split_lines(self, text, width)

# Parse command line
parser = argparse.ArgumentParser(parents=[tools.argparser], add_help = True, formatter_class = SmartFormatter)
group = parser.add_mutually_exclusive_group()
group.add_argument("-v", "--verbose", dest = "verbose", action = 'count', default = 0,
    help = "Be verbose. More -v parameters ensures more verbosity.")
group.add_argument("-q", "--quiet", dest = "verbose", action = 'store_const', const = -1, help = "Be quiet. Print only Errors.")
parser.add_argument("-l", "--log", "--logfile", dest = "logfile", metavar = "FILE", default = "-",
    help = "R|File to log to instead of logging into the stdout.\nTwo characters have a special meaning:\n"
            "':' - log to syslog\n"
            "'-' - log to stdout")
parser.add_argument("-i", "--identity", "--credentials", dest = "credentials", metavar = "FILE", default = '~/.temp_upload.json',
    required = False, help = "JSON file containing the Google Account credentials.")
parser.add_argument("-s", "--store", "--storefile", dest = "store", metavar = "FILE", default = '~/.temp_upload',
    required = False, help = "File to store Google Auth info.")
parser.add_argument("-d", "--docid", dest = "docid", metavar = "STRING", default = None,
    required = True, help = "ID of the spreadsheet to report to.")
parser.add_argument("-c", "--current_db", dest = "cdb", metavar = "FILE", default = '~/var/current.db',
    help = "Database file containing the current measured values.")
parser.add_argument("-a", "--aggr_db", dest = "adb", metavar = "FILE", default = '~/var/aggregation.db',
    help = "Database file containing the aggregated values.")
parser.add_argument("--current", dest = "current", default = False, action='store_true',
    help = "Publish current data")
parser.add_argument("--daily", dest = "daily", default = False, action='store_true',
    help = "Publish daily aggregated data")
parser.add_argument("--weekly", dest = "weekly", default = False, action='store_true',
    help = "Publish weekly aggregated data")
parser.add_argument("--monthly", dest = "monthly", default = False, action='store_true',
    help = "Publish monthly aggregated data")
parser.add_argument("--annualy", dest = "annualy", default = False, action='store_true',
    help = "Publish annualy aggregated data")
parser.add_argument("sensors", default = None, metavar='N', nargs='+',
    help = "List of sensors ID")

cmdline = parser.parse_args()

# Set the required level of logging
if cmdline.verbose < 0:
    loglevel = logging.ERROR
elif cmdline.verbose == 0:
    loglevel = logging.WARNING
elif cmdline.verbose == 1:
    loglevel = logging.INFO
else:
    loglevel = logging.DEBUG

# Logging into a file
format = '%(asctime)s %(message)s'
if cmdline.logfile == "-":
    logging.basicConfig(level = loglevel, format = format)
elif cmdline.logfile == ":":
    pass
else:
    logging.basicConfig(filename = cmdline.logfile, level = loglevel, format = format)

# Handle google library mess
###logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)

class GAuth(object):
    def __init__(self, oauth2json = None, oauth2storage = None, scope = OAUTH2_SCOPE):
        self.oauth2json = oauth2json
        self.oauth2storage = oauth2storage
        self.scope = scope
        self.store = None
        self.creds = None
        self.service = None
        logging.debug('GAuth object created')

    def auth(self, oauth2json = None, oauth2storage = None, scope = None):
        if oauth2json is not None:
            self.oauth2json = oauth2json
        if oauth2storage is not None:
            self.oauth2storage = oauth2storage
        if scope is not None:
            self.scope = scope
        if self.oauth2json is None:
            raise ValueError('Attribute oauth2json needs to be defined')
        if self.oauth2storage is None:
            raise ValueError('Attribute oauth2storage needs to be defined')
        if self.scope is None:
            raise ValueError('Attribute scope needs to be defined')

        logging.debug('Authenticating to Google, using json(%s) and store(%s)' % (self.oauth2json, self.oauth2storage))
        self.store = Storage(self.oauth2storage)
        self.creds = self.store.get()
        if self.creds is None or self.creds.invalid:
            flow = flow_from_clientsecrets(self.oauth2json, self.scope)
            self.creds = run_flow(flow, self.store, cmdline)
            self.store.put(self.creds)
        if 'spreadsheets' in self.scope.lower():
            logging.debug('Authenticating as sheets')
            discoveryUrl = ('https://sheets.googleapis.com/$discovery/rest?version=v4')
            self.service = build('sheets', 'v4', http = self.creds.authorize(Http()),
                discoveryServiceUrl = discoveryUrl,
                cache_discovery = False)
        else:
            logging.debug('Authenticating as drive')
            self.service = discovery.build('drive', 'v3', http = self.creds.authorize(httplib2.Http()))
        logging.debug('Authentication to Google is done')

def stats(stattype):
    if stattype == "current":
            current_stats()
    elif stattype == "daily": 
        daily_stats()
    elif stattype == "weekly":
            weekly_stats()
    elif stattype == "monthly":
            monthly_stats()
    elif stattype == "annualy":
            annualy_stats()

def current_stats():
    logging.info('Gathering and aggregating current data')
    sqlquery = "SELECT strftime('%Y-%m-%d %H:%M', stamputc, 'localtime') as ts, "
    sidx = 1
    for sensor in cmdline.sensors:
        sqlquery += "MAX(CASE WHEN sensor='%s' THEN temperature/1000.0 END) AS s%s, " % (sensor, str(sidx))
        sidx += 1
    sqlquery += " NULL FROM temperature WHERE stamputc >= datetime('now','-25 hours') GROUP BY ts ORDER BY stamputc DESC"
    databasefile = expanduser(cmdline.cdb)
    with sqlite3.connect(databasefile) as db:
        sql = db.cursor()
        sql.execute(sqlquery)
        data = sql.fetchone()
        values = []
        while data is not None:
            values.append(data)
            data = sql.fetchone()
        data = [{
            'range': "CURRENT!A2:%s%s" % (chr(ord('A') + len(cmdline.sensors)), len(values) + 1),
            'values': values,
        }]
        body = {
            'valueInputOption': 'RAW',
            'data': data
        }
        logging.info('Uploading current values into the spreadsheet')
        gauth.service.spreadsheets().values().batchUpdate(spreadsheetId=cmdline.docid, body=body).execute()
        logging.info('Cleaning old values in the current spreadsheet')
        ranges = {'ranges': ["CURRENT!A%s:%s" % (len(values) + 2, chr(ord('A') + len(cmdline.sensors))),]}
        gauth.service.spreadsheets().values().batchClear(spreadsheetId=cmdline.docid, body=ranges).execute()

def daily_stats():
    logging.info('Gathering daily data')
    sqlquery = "SELECT day, "
    sidx = 1
    for sensor in cmdline.sensors:
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight!=0 THEN temperature/1000.0 END) AS s%sday, " % (sensor, str(sidx))
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight=0 THEN temperature/1000.0 END) AS s%snight, " % (sensor, str(sidx))
        sidx += 1
    sqlquery += " NULL FROM daily WHERE day >= date('now','-40 days') GROUP BY day ORDER BY day DESC"
    databasefile = expanduser(cmdline.adb)
    with sqlite3.connect(databasefile) as db:
        sql = db.cursor()
        sql.execute(sqlquery)
        data = sql.fetchone()
        values = []
        while data is not None:
            values.append(data)
            data = sql.fetchone()
        data = [{
            'range': "DAILY!A2:%s%s" % (chr(ord('A') + 2*len(cmdline.sensors)), len(values) + 1),
            'values': values,
        }]
        body = {
            'valueInputOption': 'RAW',
            'data': data
        }
        logging.info('Uploading values into the daily spreadsheet')
        gauth.service.spreadsheets().values().batchUpdate(spreadsheetId=cmdline.docid, body=body).execute()
        logging.info('Cleaning old values in the daily spreadsheet')
        ranges = {'ranges': ["DAILY!A%s:%s" % (len(values) + 2, chr(ord('A') + 2*len(cmdline.sensors))),]}
        gauth.service.spreadsheets().values().batchClear(spreadsheetId=cmdline.docid, body=ranges).execute()

def weekly_stats():
    logging.info('Gathering and aggregating weekly data')
    sqlquery = "SELECT week, "
    sidx = 1
    for sensor in cmdline.sensors:
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight!=0 THEN temperature/1000.0 END) AS s%sday, " % (sensor, str(sidx))
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight=0 THEN temperature/1000.0 END) AS s%snight, " % (sensor, str(sidx))
        sidx += 1
    sqlquery += " NULL FROM weekly WHERE week >= strftime('%Y-%W', date('now','-1 year'), 'localtime') GROUP BY week ORDER BY week DESC"
    databasefile = expanduser(cmdline.adb)
    with sqlite3.connect(databasefile) as db:
        sql = db.cursor()
        sql.execute(sqlquery)
        data = sql.fetchone()
        values = []
        while data is not None:
            values.append(data)
            data = sql.fetchone()
        data = [{
            'range': "WEEKLY!A2:%s%s" % (chr(ord('A') + 2*len(cmdline.sensors)), len(values) + 1),
            'values': values,
        }]
        body = {
            'valueInputOption': 'RAW',
            'data': data
        }
        logging.info('Uploading values into the weekly spreadsheet')
        gauth.service.spreadsheets().values().batchUpdate(spreadsheetId=cmdline.docid, body=body).execute()
        logging.info('Cleaning old values in the weekly spreadsheet')
        ranges = {'ranges': ["WEEKLY!A%s:%s" % (len(values) + 2, chr(ord('A') + 2*len(cmdline.sensors))),]}
        gauth.service.spreadsheets().values().batchClear(spreadsheetId=cmdline.docid, body=ranges).execute()

def monthly_stats():
    logging.info('Gathering and aggregating monthly data')
    sqlquery = "SELECT month, "
    sidx = 1
    for sensor in cmdline.sensors:
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight!=0 THEN temperature/1000.0 END) AS s%sday, " % (sensor, str(sidx))
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight=0 THEN temperature/1000.0 END) AS s%snight, " % (sensor, str(sidx))
        sidx += 1
    sqlquery += " NULL FROM monthly GROUP BY month ORDER BY month DESC"
    databasefile = expanduser(cmdline.adb)
    with sqlite3.connect(databasefile) as db:
        sql = db.cursor()
        sql.execute(sqlquery)
        data = sql.fetchone()
        values = []
        while data is not None:
            values.append(data)
            data = sql.fetchone()
        data = [{
            'range': "MONTHLY!A2:%s%s" % (chr(ord('A') + 2*len(cmdline.sensors)), len(values) + 1),
            'values': values,
        }]
        body = {
            'valueInputOption': 'RAW',
            'data': data
        }
        logging.info('Uploading values into the monthly spreadsheet')
        gauth.service.spreadsheets().values().batchUpdate(spreadsheetId=cmdline.docid, body=body).execute()
        logging.info('Cleaning old values in the monthly spreadsheet')
        ranges = {'ranges': ["MONTHLY!A%s:%s" % (len(values) + 2, chr(ord('A') + 2*len(cmdline.sensors))),]}
        gauth.service.spreadsheets().values().batchClear(spreadsheetId=cmdline.docid, body=ranges).execute()

def annualy_stats():
    logging.info('Gathering and aggregating annualy data')
    sqlquery = "SELECT year, "
    sidx = 1
    for sensor in cmdline.sensors:
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight!=0 THEN temperature/1000.0 END) AS s%sday, " % (sensor, str(sidx))
        sqlquery += "MAX(CASE WHEN sensor='%s' and daylight=0 THEN temperature/1000.0 END) AS s%snight, " % (sensor, str(sidx))
        sidx += 1
    sqlquery += " NULL FROM annualy GROUP BY year ORDER BY year DESC"
    databasefile = expanduser(cmdline.adb)
    with sqlite3.connect(databasefile) as db:
        sql = db.cursor()
        sql.execute(sqlquery)
        data = sql.fetchone()
        values = []
        while data is not None:
            values.append(data)
            data = sql.fetchone()
        data = [{
            'range': "ANNUALY!A2:%s%s" % (chr(ord('A') + 2*len(cmdline.sensors)), len(values) + 1),
            'values': values,
        }]
        body = {
            'valueInputOption': 'RAW',
            'data': data
        }
        logging.info('Uploading values into the annualy spreadsheet')
        gauth.service.spreadsheets().values().batchUpdate(spreadsheetId=cmdline.docid, body=body).execute()
        logging.info('Cleaning old values in the annualy spreadsheet')
        ranges = {'ranges': ["ANNUALY!A%s:%s" % (len(values) + 2, chr(ord('A') + 2*len(cmdline.sensors))),]}
        gauth.service.spreadsheets().values().batchClear(spreadsheetId=cmdline.docid, body=ranges).execute()


oauth2json = expanduser(cmdline.credentials)
oauth2storage = expanduser(cmdline.store)
logging.info('Authenticating to Google')
gauth = GAuth(oauth2json = oauth2json, oauth2storage = oauth2storage)
gauth.auth()

#with Pool(5) as p:
#    p.map(stats, ["current", "daily", "weekly", "monthly", "annualy"])
if cmdline.current:
    current_stats()
if cmdline.daily:
    daily_stats()
if cmdline.weekly:
    weekly_stats()
if cmdline.monthly:
    monthly_stats()
if cmdline.annualy:
    annualy_stats()



